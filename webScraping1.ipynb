from bs4 import BeautifulSoup as bs
from pprint import pprint
from urllib.request import urlopen
import requests

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

from datetime import datetime

# 헤더 : User-Agent 설정
headers = {'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36'}
# 개발자 도구의 console 차에 navigator.userAgent 쳐서 확인


# 웹드라이버 설치
options = Options()
options.add_argument('--no-sandbox')
options.add_argument('--headless')
options.add_argument('--disable-dev-shm-usage')

options.add_argument("start-maximized"); 
options.add_argument("disable-infobars"); 
options.add_argument("--disable-extensions"); 
options.add_argument("--disable-gpu"); 
options.add_argument("--disable-dev-shm-usage"); 

# driver = webdriver.Chrome(options=options)
# # 25.01.20. replit에서 웹드라이버 설치 요령 터득!
# # https://stackoverflow.com/questions/71201650/how-to-use-selenium-on-repl-it

# url = 'https://store.kyobobook.co.kr/bestseller/online/daily'
# driver.get(url)

# with webdriver.Chrome(options=options) as driver:
#     driver.get("https://store.kyobobook.co.kr/bestseller/online/daily")
#     element = WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.XPATH, "/html/body/div[1]/main/section/div/div/section/ol[1]/li[1]/div/div[2]/div[2]/a")))
#     print(element.text)

page = 1
url='https://store.kyobobook.co.kr/bestseller/online/daily'

req = requests.get(url)
content = req.content
soup = bs(content, 'html.parser')

book_names = soup.select('div.ml-4.min-w-[516px].w-full')
book_names[0].text

# 25.01.20 https://boringariel.tistory.com/64 야 내가 딱 원하는 웹크롤러 나왔다 이건 대충 훑어보고 내일 마저 보기로 함

# 25.01.20 왜 자꾸 빈 배열 반환 쳐 하는데 씨!@빨 !!
# 그니까 문제가 뭐냐하면, 너무 많은 태그의 태그의 태그 ... 속에 파묻힌 작고 소중한 텍스트 알맹이를 긁어와야 하는데 긁어오려는 시도마다 빈 배열을 쳐@반환하고 있다는 것이다
# 해결방법을 졸라게 찾고 있따. 
# bookName = driver.find_elements(By.XPATH, links_Xpath)
# print(bookName)

# driver.quit()

# Xpath : /html/body/div[1]/main/section/div/div/section/ol[1]/li[1]/div/div[2]/div[2]/a
# full Xpath : /html/body/div[1]/main/section/div/div/section/ol[1]/li[1]/div/div[2]/div[2]/a

# 25.01.20 여기까지 했는데 에러뜬다.. 스오플에 찾아보니 https://stackoverflow.com/questions/78264537/error-with-selector-using-webdriver-in-python
# 이런 XPath를 쓰는 해결 방안이 있다고 하는데, 내일 마저 알아보자.
# new_text = "도서"
# driver.execute_script("arguments[0].innerText = arguments[1];", a_tag, new_text)

# print(a_tag.text)
# driver.quit()

# 오늘 날짜
# dateToday = datetime.today().strftime("%Y.%m.%d.")

# class="prod_link font-weight-medium line-clamp-2 text-black hover:underline   font-text-xl mt-2"
# html = requests.get(url)
# # pprint(html.text)

# html = requests.get(url, headers = headers)
# soup = bs(html.content, 'html.parser')

# data1 = soup.findAll('a', class_='prod_link font-weight-medium line-clamp-2 text-black hover:underline   font-text-xl mt-2')
# # target_div = soup.select('div.keyword_rank span')
# # span 태그의 title_cell 클래스의 내용과 '오늘' 날짜값이 일치하면 해당 내용을 출력하게 하고 싶은데.
# pprint(data1)
# pprint(target_div)

# 25.01.20. 네이버 데이터랩 홈의 도서 - rankList는 대체 어떻게 돼먹은 구조인지 도통 알 수 없어서 포기하고,
# 원래 진행하려고 했던 교보문고 웹사이트 분석 재시도


# 웹 페이지는 HTML이라는 언어로 쓰여져있습니다. 이를 파이썬에서 쉽게 분석할 수 있도록 파싱작업을 거쳐 각 요소에 접근이 쉽게 만듭니다.


# 그러려 먼 간단한 설정을 해주어야 하는데... 이유는 네이버에서 크롤링을 막았다고 해야 하나...? 
# 아무튼 위의 방법대로 접근을 해보면 빈 배열만 나오고 크롤링이 안됨 -> 네이버, 교보문고 등 큰 웹사이트들은 크롤링을 막은듯

# soup = bs(html.text, 'html.parser')

# data1 = soup.findAll('div', {'class':'ml-4 min-w-[516px] w-full'})
# pprint(data1)

# data2 = data1.find('strong')
# pprint(data2)
